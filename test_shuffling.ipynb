{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de35622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.auto as tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c3ef40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5950"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "data_files = \"/local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/**/*.parquet\"\n",
    "\n",
    "ddf = dd.read_parquet(data_files, engine='pyarrow')\n",
    "df = ddf\n",
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a49063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "regex = re.compile(r\".*/quality=(?P<quality>[a-z-]+)/kind=(?P<kind>[a-z-]+)/kind2=(?P<kind2>[a-z-_]+)/CC-MAIN-(?P<year>\\d+)-(?P<month>\\d+)-part-(?P<part>\\d+)(\\.parquet|\\.jsonl\\.zstd)$\")\n",
    "\n",
    "def parse_filename(file):\n",
    "    match = re.search(regex, file)\n",
    "    if match:\n",
    "        meta = match.groupdict()\n",
    "        meta['part'] = int(meta['part'])\n",
    "        meta['year'] = int(meta['year'])\n",
    "        meta['month'] = int(meta['month'])\n",
    "        return meta\n",
    "    print(file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9b93cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5005 5950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('high', 'actual', 'actual'): (404, '8.07%'),\n",
       " ('high', 'synthetic', 'distill'): (135, '2.70%'),\n",
       " ('high', 'synthetic', 'diverse_qa_pairs'): (398, '7.95%'),\n",
       " ('high', 'synthetic', 'extract_knowledge'): (254, '5.07%'),\n",
       " ('high', 'synthetic', 'knowledge_list'): (150, '3.00%'),\n",
       " ('high', 'synthetic', 'wrap_medium'): (274, '5.47%'),\n",
       " ('low', 'actual', 'actual'): (298, '5.95%'),\n",
       " ('low', 'synthetic', 'wrap_medium'): (263, '5.25%'),\n",
       " ('medium', 'actual', 'actual'): (1614, '32.25%'),\n",
       " ('medium-high', 'actual', 'actual'): (396, '7.91%'),\n",
       " ('medium-low', 'actual', 'actual'): (819, '16.36%')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "files = glob.glob(data_files, recursive=True)\n",
    "file_meta = [parse_filename(file) for file in files]\n",
    "\n",
    "sorted_meta, sorted_files = zip(*sorted(zip(file_meta, files), key=lambda x: (x[0][\"quality\"], x[0][\"kind\"], x[0][\"kind2\"], x[0]['year'], x[0]['month'], x[0]['part'])))\n",
    "\n",
    "grouped_files = defaultdict(list)\n",
    "for meta, file in zip(sorted_meta, sorted_files):\n",
    "    grouped_files[(meta['quality'], meta['kind'], meta['kind2'])].append(file)\n",
    "\n",
    "grouped_files[(\"medium\", \"actual\", \"actual\")] = grouped_files[(\"medium\", \"actual\", \"actual\")][:int(0.75*len(grouped_files[(\"medium\", \"actual\", \"actual\")]))]\n",
    "grouped_files[(\"medium-high\", \"actual\", \"actual\")] = grouped_files[(\"medium-high\", \"actual\", \"actual\")][:int(0.75*len(grouped_files[(\"medium-high\", \"actual\", \"actual\")]))]\n",
    "grouped_files[(\"medium-low\", \"actual\", \"actual\")] = grouped_files[(\"medium-low\", \"actual\", \"actual\")][:int(0.75*len(grouped_files[(\"medium-low\", \"actual\", \"actual\")]))]\n",
    "\n",
    "total = sum(len(vals) for vals in grouped_files.values())\n",
    "print(total, len(files))\n",
    "{key: (len(vals), f\"{100*len(vals) / total:.2f}%\") for key, vals in grouped_files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b1632f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = [f for files in grouped_files.values() for f in files]\n",
    "\n",
    "with open(\"nemotron-cc-split.txt\", \"w\") as f:\n",
    "    f.writelines(f\"{line}\\n\" for line in split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5386b136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('high', 'actual', 'actual'): (330, '6.60%'),\n",
       " ('high', 'synthetic', 'distill'): (110, '2.20%'),\n",
       " ('high', 'synthetic', 'diverse_qa_pairs'): (326, '6.52%'),\n",
       " ('high', 'synthetic', 'extract_knowledge'): (204, '4.08%'),\n",
       " ('high', 'synthetic', 'knowledge_list'): (122, '2.44%'),\n",
       " ('high', 'synthetic', 'wrap_medium'): (227, '4.54%'),\n",
       " ('low', 'actual', 'actual'): (251, '5.02%'),\n",
       " ('low', 'synthetic', 'wrap_medium'): (222, '4.44%'),\n",
       " ('medium', 'actual', 'actual'): (1808, '36.16%'),\n",
       " ('medium-high', 'actual', 'actual'): (456, '9.12%'),\n",
       " ('medium-low', 'actual', 'actual'): (944, '18.88%')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "shuffled_files = random.sample(files, 5000)\n",
    "shuffled_files_meta = [parse_filename(file) for file in shuffled_files]\n",
    "\n",
    "sorted_meta, sorted_files = zip(*sorted(zip(shuffled_files_meta, shuffled_files), key=lambda x: (x[0][\"quality\"], x[0][\"kind\"], x[0][\"kind2\"], x[0]['year'], x[0]['month'], x[0]['part'])))\n",
    "\n",
    "grouped_files = defaultdict(list)\n",
    "for meta, file in zip(sorted_meta, sorted_files):\n",
    "    grouped_files[(meta['quality'], meta['kind'], meta['kind2'])].append(file)\n",
    "\n",
    "{key: (len(vals), f\"{100*len(vals) / len(shuffled_files):.2f}%\") for key, vals in grouped_files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa43d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = ddf.partitions[0][\"tokens\"].map(len, meta=(\"len\", \"int64\")).sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beeae967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "max_num_files = 5000\n",
    "\n",
    "all_jsonls = sorted(glob.glob(f\"/pub/hofmann-scratch/data/Nemotron-CC/**/*.jsonl.zstd\", recursive=True))\n",
    "rng = random.Random(0)\n",
    "rng.shuffle(all_jsonls)\n",
    "if max_num_files is not None:\n",
    "    all_jsonls = all_jsonls[:max_num_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ea6691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('high', 'actual', 'actual'): (428, '8.56%'),\n",
       " ('high', 'synthetic', 'distill'): (141, '2.82%'),\n",
       " ('high', 'synthetic', 'diverse_qa_pairs'): (428, '8.56%'),\n",
       " ('high', 'synthetic', 'extract_knowledge'): (265, '5.30%'),\n",
       " ('high', 'synthetic', 'knowledge_list'): (158, '3.16%'),\n",
       " ('high', 'synthetic', 'wrap_medium'): (293, '5.86%'),\n",
       " ('low', 'actual', 'actual'): (311, '6.22%'),\n",
       " ('low', 'synthetic', 'wrap_medium'): (283, '5.66%'),\n",
       " ('medium', 'actual', 'actual'): (1622, '32.44%'),\n",
       " ('medium-high', 'actual', 'actual'): (378, '7.56%'),\n",
       " ('medium-low', 'actual', 'actual'): (693, '13.86%')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_files = [f.replace(\"/pub/hofmann-scratch/data/Nemotron-CC\", \"/local/home/dvruette/nemotron_tokenized\").replace(\"data-jsonl\", \"data\").replace(\".jsonl.zstd\", \".parquet\") for f in all_jsonls]\n",
    "\n",
    "sampled_file_meta = [parse_filename(file) for file in sampled_files]\n",
    "\n",
    "sorted_sampled_meta, sorted_sampled_files = zip(*sorted(zip(sampled_file_meta, sampled_files), key=lambda x: (x[0][\"quality\"], x[0][\"kind\"], x[0][\"kind2\"], x[0]['year'], x[0]['month'], x[0]['part'])))\n",
    "\n",
    "grouped_sampled_files = defaultdict(list)\n",
    "for meta, file in zip(sorted_sampled_meta, sorted_sampled_files):\n",
    "    grouped_sampled_files[(meta['quality'], meta['kind'], meta['kind2'])].append(file)\n",
    "\n",
    "{key: (len(vals), f\"{100*len(vals) / len(sampled_files):.2f}%\") for key, vals in grouped_sampled_files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22426bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('high', 'synthetic', 'diverse_qa_pairs'): 1,\n",
       " ('medium-high', 'actual', 'actual'): 167,\n",
       " ('medium-low', 'actual', 'actual'): 421,\n",
       " ('medium', 'actual', 'actual'): 609}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_files = sorted(set(files) - set(sampled_files))\n",
    "\n",
    "stats = {}\n",
    "for file in extra_files:\n",
    "    meta = parse_filename(file)\n",
    "    key = (meta['quality'], meta['kind'], meta['kind2'])\n",
    "    if key not in stats:\n",
    "        stats[key] = 0\n",
    "    stats[key] += 1\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "834cf3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('high', 'actual', 'actual'): (395, '8.57%'),\n",
       " ('high', 'synthetic', 'distill'): (130, '2.82%'),\n",
       " ('high', 'synthetic', 'diverse_qa_pairs'): (390, '8.46%'),\n",
       " ('high', 'synthetic', 'extract_knowledge'): (241, '5.23%'),\n",
       " ('high', 'synthetic', 'knowledge_list'): (147, '3.19%'),\n",
       " ('high', 'synthetic', 'wrap_medium'): (261, '5.66%'),\n",
       " ('low', 'actual', 'actual'): (291, '6.32%'),\n",
       " ('low', 'synthetic', 'wrap_medium'): (257, '5.58%'),\n",
       " ('medium-high', 'actual', 'actual'): (353, '7.66%'),\n",
       " ('medium-low', 'actual', 'actual'): (650, '14.11%'),\n",
       " ('medium', 'actual', 'actual'): (1493, '32.40%')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_files = sorted(set(files) & set(sampled_files[:4612]))\n",
    "\n",
    "stats = {}\n",
    "for file in existing_files:\n",
    "    meta = parse_filename(file)\n",
    "    key = (meta['quality'], meta['kind'], meta['kind2'])\n",
    "    if key not in stats:\n",
    "        stats[key] = 0\n",
    "    stats[key] += 1\n",
    "\n",
    "print(len(existing_files))\n",
    "{key: (val, f\"{100*val / len(existing_files):.2f}%\") for key, val in stats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd7f2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('high', 'actual', 'actual'): (2755, '8.81%'),\n",
       " ('high', 'synthetic', 'distill'): (939, '3.00%'),\n",
       " ('high', 'synthetic', 'diverse_qa_pairs'): (2564, '8.20%'),\n",
       " ('high', 'synthetic', 'extract_knowledge'): (1694, '5.42%'),\n",
       " ('high', 'synthetic', 'knowledge_list'): (1140, '3.64%'),\n",
       " ('high', 'synthetic', 'wrap_medium'): (2016, '6.45%'),\n",
       " ('low', 'actual', 'actual'): (1964, '6.28%'),\n",
       " ('low', 'synthetic', 'wrap_medium'): (1788, '5.72%'),\n",
       " ('medium', 'actual', 'actual'): (9678, '30.94%'),\n",
       " ('medium-high', 'actual', 'actual'): (2454, '7.85%'),\n",
       " ('medium-low', 'actual', 'actual'): (4287, '13.71%')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_files = glob.glob(\"/pub/hofmann-scratch/data/Nemotron-CC/**/*.jsonl.zstd\", recursive=True)\n",
    "orig_file_meta = [parse_filename(file) for file in orig_files]\n",
    "\n",
    "sorted_orig_meta, sorted_orig_files = zip(*sorted(zip(orig_file_meta, orig_files), key=lambda x: (x[0][\"quality\"], x[0][\"kind\"], x[0][\"kind2\"], x[0]['year'], x[0]['month'], x[0]['part'])))\n",
    "\n",
    "grouped_orig_files = defaultdict(list)\n",
    "for meta, file in zip(sorted_orig_meta, sorted_orig_files):\n",
    "    grouped_orig_files[(meta['quality'], meta['kind'], meta['kind2'])].append(file)\n",
    "\n",
    "{key: (len(vals), f\"{100*len(vals) / len(orig_files):.2f}%\") for key, vals in grouped_orig_files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb4c1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = sorted(set(files) & set(sampled_files[:4612]))\n",
    "assert len(split) == 4608  # 4096 + 512; ~1T tokens\n",
    "\n",
    "with open(\"nemotron-cc-split.txt\", \"w\") as f:\n",
    "    f.writelines(f\"{line}\\n\" for line in split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fb6b187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e110ad5d54145489f2f38fbafe404ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 3 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "# delete files not in split\n",
    "\n",
    "remaining_files = sorted(glob.glob(f\"/local/home/dvruette/nemotron_tokenized/**/*.parquet\", recursive=True))\n",
    "\n",
    "\n",
    "split_files = []\n",
    "with open(\"nemotron-cc-split.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        split_files.append(line.strip())\n",
    "\n",
    "num_deleted = 0\n",
    "num_remaining = len(set(remaining_files) - (set(remaining_files) - set(split_files)))\n",
    "assert num_remaining == 4608, f\"Expected 4608 files, found {num_remaining}\"\n",
    "for file in tqdm.tqdm(list(set(remaining_files) - set(split_files))):\n",
    "    # print(f\"Deleting {file}\")\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        num_deleted += 1\n",
    "        pass\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file} not found, skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file}: {e}\")\n",
    "\n",
    "print(f\"Deleted {num_deleted} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eac972f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4611"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_files = sorted(glob.glob(f\"/local/home/dvruette/nemotron_tokenized/**/*.parquet\", recursive=True))\n",
    "len(remaining_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cda78750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4608"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split_files = []\n",
    "with open(\"nemotron-cc-split.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        split_files.append(line.strip())\n",
    "\n",
    "len(split_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edc1dcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'/local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=high/kind=synthetic/kind2=distill/CC-MAIN-2013-20-part-00003.parquet',\n",
       "  '/local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=medium/kind=actual/kind2=actual/CC-MAIN-2015-06-part-00031.parquet',\n",
       "  '/local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=medium/kind=actual/kind2=actual/CC-MAIN-2024-30-part-00005.parquet'},\n",
       " set())"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(remaining_files) - set(split_files), set(split_files) - set(remaining_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "06c06b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ab5ee1ebf2455bbf6de494ae5508ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1384: 213888153\n",
      "625 : 217872431\n",
      "3795: 224092477\n",
      "754 : 208462859\n",
      "978 : 194725227\n",
      "926 : 192144100\n",
      "2590: 221744631\n",
      "2597: 224437231\n",
      "1948: 225692886\n",
      "4080: 222484555\n",
      "2833: 231279686\n",
      "2064: 227719733\n",
      "256 : 222107057\n",
      "4022: 221265877\n",
      "737 : 209203770\n",
      "2145: 218691504\n",
      "mean: 217238261.0625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tokens_per_partition = []\n",
    "\n",
    "ddf = dd.read_parquet(existing_files, engine='pyarrow')\n",
    "# select 16 random partitions\n",
    "ids = [np.random.choice(ddf.npartitions, replace=False) for _ in range(16)]\n",
    "for i in tqdm.tqdm(ids):\n",
    "    total = ddf.partitions[i][\"tokens\"].map(len, meta=(\"len\", \"int64\")).sum().compute()\n",
    "    print(f\"{i:<4}: {total}\")\n",
    "    tokens_per_partition.append(total)\n",
    "\n",
    "print(f\"mean: {np.mean(tokens_per_partition)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3fc8477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=medium-low/kind=actual/kind2=actual/CC-MAIN-2014-10-part-00000.parquet',\n",
       " '/home/loca/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=low/kind=synthetic/kind2=wrap_medium/CC-MAIN-2023-14-part-00013.parquet')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0], sampled_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e07ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86034cfbfb73402d98644d429ce12346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534 : 214830047\n",
      "2839: 221560310\n",
      "1388: 210411937\n",
      "2969: 225110990\n",
      "3864: 230857967\n",
      "58  : 222430617\n",
      "2751: 223562157\n",
      "4983: 226629966\n",
      "2745: 222297603\n",
      "483 : 210296546\n",
      "4224: 226364975\n",
      "742 : 209149153\n",
      "3894: 230174659\n",
      "2862: 224298112\n",
      "4379: 227973104\n",
      "3277: 219671421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(221601222.75)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tokens_per_partition = []\n",
    "# select 16 random partitions\n",
    "ids = [np.random.choice(ddf.npartitions, replace=False) for _ in range(16)]\n",
    "for i in tqdm.tqdm(ids):\n",
    "    total = ddf.partitions[i][\"tokens\"].map(len, meta=(\"len\", \"int64\")).sum().compute()\n",
    "    print(f\"{i:<4}: {total}\")\n",
    "    tokens_per_partition.append(total)\n",
    "\n",
    "print(f\"mean: {np.mean(tokens_per_partition)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cff9b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(220234989.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(tokens_per_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8912f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f924b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gidd_easydel.sampler import BufferedPartitionSampler\n",
    "\n",
    "sampler = BufferedPartitionSampler(\n",
    "    ddf, K=16, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb3b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    yield from sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ebef95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import IterableDataset\n",
    "\n",
    "ds = IterableDataset.from_generator(generate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9879637d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': array([ 24467,     80,  12081,   2093,   8333,     56,  32565,    194,\n",
       "            27,   5924,   7274,    194,     27,  55970,   2851,   4695,\n",
       "         66021,    370,    806,    341,    893,    361,    292,    791,\n",
       "          6203,    253,   1483,    399,   1240,    291,   2899,   2937,\n",
       "           484,    274,    864,   6203,    253,    250,     87,     84,\n",
       "           922,    550,   3259,    287,   4483,    291,    274,    864,\n",
       "           908,   6203,    253,    550,   1379,    308,    806,    341,\n",
       "          4514,   9675,  15791,  38793,    484,   1592,   4728,   4149,\n",
       "           593,   2937,    308,    806,    341,  58390,    361,    292,\n",
       "          1302,    246,   4018,    426,    662,    340,   5320,    484,\n",
       "           274,    421,    439,   3866,   2062,    308,    806,    341,\n",
       "           893,    411,    399,   9760,   7623,    484,   3161,   5843,\n",
       "          4234,   1483,   4296,    308,    806,    341,   4784,    697,\n",
       "           292,    640,    654,    250,     87,     84,     97,     84,\n",
       "           688,    346,    572,    291,   2294,  12803,    484,    274,\n",
       "          2856,   2688,    530,   7079,    194,     27,    274,    697,\n",
       "          3314,    425,    530,   2060,    246,   1451,    308,    806,\n",
       "         75356,    287,    399,   9760,   9434,  81362,   3174,    484,\n",
       "           274,    421,    844,    194,     29,    323,    818,    530,\n",
       "           287,  11983,    194,     29,    274,   4234,    550,   8240,\n",
       "          9757,   1582,   5396,   3736,    194,     29,    323,   1654,\n",
       "           530,    287,   8697,    194,     29,    274,   1333,   2396,\n",
       "          4679,    253,   2905,    291,   6941,   5894,    308,    806,\n",
       "         75356,    287,    292,   9760,   4655,    291,   5565,    401,\n",
       "          6651,   1483,    250,     87,     84,   1053,  15173,    274,\n",
       "          1333,    982,  37859,  13386,    268,  10451,    590,    401,\n",
       "           315,    345,   4364,   1172,    250,     87,     84,    706,\n",
       "           194,     29,  21697,  20757,   1035,    412,   1898,    316,\n",
       "          2423,  39207,   5723,    386,    289,   1635,   1691,    194,\n",
       "            29,   1584,   9760,   5565,    697,    281,    530,   1542,\n",
       "         52236,    429,    250,     87,     84,   2737,    194,     27,\n",
       "           194,    793,   2414,    425,    550,   1750,    194,     29,\n",
       "           582,    287,    250,     87,     84,   1114,    892,    274,\n",
       "          4417,    316,   2357,    291,    892,    274,    704,    250,\n",
       "            87,     84,   2624,    291,   2754,    839,   6253,    308,\n",
       "           806,    341,    893,    287,    399,   9760,  25541,    484,\n",
       "           274,    420,    439,    421,    530,   1214,    194,     29,\n",
       "           274,   1489,    439,    246,    666,   1048,   7634,    246,\n",
       "          1407,    308,    806,    341,   3440,   3175,    420,    292,\n",
       "           791,   1318,    861,    194,     94,     93,    399,  21341,\n",
       "           484,   3645,    194,     27,    328,   6739,    194,     94,\n",
       "            93,    250,     87,     84,   8387,    194,     29,  23097,\n",
       "           274,   6399,    246,   1985,    194,     94,     93,    250,\n",
       "            87,     84,   5285,   1483,    550,    902,    194,     27,\n",
       "           331,    274,   4018,    289,   1324,    253,    550,    572,\n",
       "          4230,    194,     29,    274,    421,    530,    194,     27,\n",
       "           844,   9760,   5285,   9668,    291,    274,    585,    646,\n",
       "           246,   7200,    442,    316,   4216,    253,   6168,    308,\n",
       "           806,    341,   3440,    650,    253,    250,     87,     84,\n",
       "          1035,    291,    253,    250,     87,     84,   1632,    287,\n",
       "           399,   9760,    484,   1590,    697,   4036,    281,   4386,\n",
       "          3370,    401,    664,    274,    420,    439,    751,    194,\n",
       "            94,     93,   4386,    194,     27,    315,    287,  21250,\n",
       "           323,   2696,    287,   2201,    341,    250,     87,     84,\n",
       "          1714,   1632,    287,    645,    291,   4355,   2944,    246,\n",
       "           281,    372,   1807,   1685,    194,     29,    274,   1163,\n",
       "         31701,    194,     27,    504,    274,   8344,    246,   1207,\n",
       "           426,    922,    985,    274,    411,   1483,   1307,    308,\n",
       "           806,  75356,    287,    399,   9760,   1425,    253,   4102,\n",
       "           484,   1991,    308,    806,    341,    829,    292,    854,\n",
       "          2847,    246,    250,     87,     84,   1965,    194,     94,\n",
       "            97,  22483,    692,    250,     87,     84,   1915,    194,\n",
       "            27,    784,   1875,    697,    292,    585,    246,    281,\n",
       "           484,    274,    864,   3373,    892,    274,    864,    967,\n",
       "           194,     29,   1100,    287,   2919,   1483,    250,     87,\n",
       "            84,   1965,    315,    274,    697,    420,  10747,    291,\n",
       "           274,   5013,    539,    585,    246,    723,    550,   1915,\n",
       "          1483,   4471,    308,    806,  97654,    292,    854,   8884,\n",
       "           642,   1717,   1483,   2149,    580,    530,    697,    292,\n",
       "          2688,    484,    274,   3184,    274,    854,    421,    681,\n",
       "          3252,   1483,    250,     87,     84,  11741,    253,    250,\n",
       "            87,     84,   8815,  17385,   4890,   9997,   1483,   3744,\n",
       "           194,     29,    586,    411,  15206,    308,    806,  75356,\n",
       "           411,    250,     87,     84,   1026,   3258,    292,    421,\n",
       "          8913,    484,   1584,   6102,    340,   4563,    308,    806,\n",
       "         37801,   1280,   2664,    194,     27,   1527,    194,     27,\n",
       "         10646,    484,    323,   1280,   2664,    274,   3422,    411,\n",
       "           250,     87,     84,    530,    274,    421,   9100,   1786,\n",
       "          1765,    899,    323,  48286,    253,  30183,    316,   3030,\n",
       "          6578,    401,    550,   3616,   1196,   4478,   1527,    899,\n",
       "           323,  15524,    429,  98017,   3294,  10719,   3518,    194,\n",
       "            29,  27124,    899,  95826,   1138,   7967,  68423,    194,\n",
       "            27,    580,    839,    662,    287,  15613,   3092,    194,\n",
       "            94,     93,  23996,    735,    486,   2447,    374,  45401,\n",
       "         13775,    374,   1483,  54571,    194,     27,    530,    253,\n",
       "           250,     87,     84,    791,   4579,   3451,   7779,    194,\n",
       "            94,     93,    250,     87,     84,  65760,   6240,    308,\n",
       "           806,  75356,  10885,   2173,    697,    292,    281,    484,\n",
       "         17158,  24045,    899,   2336,   1483,    993,   4711,  11043,\n",
       "           664,    349,    291,   7228,   3247,   5376,    295,   1057,\n",
       "           655,    340,    810,   6627,    308,    806,  75356,    991,\n",
       "           292,   2198,   1280,   1632,    484,   1590,   2905,    461,\n",
       "          1569,   3081,   8476,    944,    194,     29,    274,   3422,\n",
       "           289,   9762,    253,    289,   2727,   3084,   7455,   3721,\n",
       "           523,    253,    290,  16755,   3084,    308,  24467,     80,\n",
       "         40849,   1446,  60094,    370,  24467,     80,  40849,   1446,\n",
       "         12710,    194,     27,   9293,  12236,    370,   1358,   2457,\n",
       "           569,   3259,   1483,  12283,    316,    250,     87,     84,\n",
       "          1328,    253,    194,   1841,    194,     27,   1714,   1172,\n",
       "         27623,   1483,   9314,    291,   6758,   1701,  50567,  45931,\n",
       "          6240,    194,     27,    291,   2254,    331,    289,   2962,\n",
       "           194,     27,   1483,   6241,    308,   1491,    250,     87,\n",
       "            84,   1280,    662,    253,    569,   3688,    858,   1210,\n",
       "           289,   3451,   1711,   1483,    289,   1243,   3654,   2389,\n",
       "          1483,  54571,    899,   3654,   2798,    619,   2036,  20885,\n",
       "          1111,  11773,   6902,   2646,    194,     27,    504,    426,\n",
       "          4193,    523,    246,    281,    439,    807,    289,   3451,\n",
       "          1711,    194,     29,   1358,   9147,   1483,    250,     87,\n",
       "            84,   1433,    295,    194,    693,    925,    194,     27,\n",
       "          3512,    331,    289,  16551,    194,     27,   8382,    553,\n",
       "           331,    289,   7044,    253, 124795,   4154,    194,     29,\n",
       "          7775,    289,    908,    938,   2962,   4138,    569,    290,\n",
       "          2776,    246,   1391,    289,  13760,    295,  35355,   3688,\n",
       "          1483,   4296,    194,     27,    892,    858,   1014,    547,\n",
       "           908,   3036,    763,    291,   4800,    246,    646,   7184,\n",
       "          1483,   1333,    316, 124795,  12283,    194,     29,   1901,\n",
       "           925,    253,   1127,   1023,   1483,    250,     87,     84,\n",
       "          3654,   2155,    194,     27,   1358,   3866,    246,   1782,\n",
       "           246,  66021,    194,     29,   4342,    858,    751,    331,\n",
       "           289,   5924,   5653,   1483,   8846,    262,   4695,  66021,\n",
       "          1483,  54571,    194,     27,    530,    253,    250,     87,\n",
       "            84,   2506,  13462,  18179,   1483,    250,     87,     84,\n",
       "          2060,    194,     27,    291,    858,    287,   1236,    908,\n",
       "         10034,    584, 124795,  12283,    580,    858,   1704,   1797,\n",
       "           331,    650,    253,    569,  13417,   2474], dtype=int32),\n",
       " 'quality': 'low',\n",
       " 'kind': 'actual',\n",
       " 'kind2': 'actual'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285dfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_rand_key(pdf, *, seed=0, partition_info=None, as_uint64=True):\n",
    "    \"\"\"\n",
    "    Add a random key column to a pandas DataFrame partition (pdf).\n",
    "    Random stream is deterministic per partition: seed + partition_id.\n",
    "    \"\"\"\n",
    "    pid = 0 if partition_info is None else partition_info[\"number\"]\n",
    "    rng = np.random.default_rng(seed + pid)\n",
    "\n",
    "    if as_uint64:\n",
    "        key = rng.integers(\n",
    "            low=0,\n",
    "            high=np.iinfo(np.uint64).max,\n",
    "            size=len(pdf),\n",
    "            dtype=np.uint64,\n",
    "            endpoint=True,  # include max value\n",
    "        )\n",
    "        return pdf.assign(rand_key=key)\n",
    "    else:\n",
    "        # If you prefer float keys in [0, 1)\n",
    "        return pdf.assign(rand_key=rng.random(len(pdf)))\n",
    "\n",
    "# Tell Dask the dtype of the new column via meta\n",
    "meta_uint64 = ddf._meta.assign(rand_key=np.uint64(0))\n",
    "\n",
    "ddf_with_key = ddf.map_partitions(\n",
    "    add_rand_key,\n",
    "    seed=123456,                 # base seed (change per epoch if you like)\n",
    "    as_uint64=True,              # set False to use float keys\n",
    "    meta=meta_uint64,            # ensures correct dtype without a full compute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ba5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_sorted = ddf_with_key.set_index(\n",
    "    \"rand_key\",\n",
    "    # npartitions=256,     # tune for your cluster\n",
    "    shuffle=\"disk\",       # scalable disk-based shuffle\n",
    "    sort=True            # compute divisions via sampling\n",
    ")\n",
    "\n",
    "# 2) Ensure rows are ordered within each partition (set_index guarantees global\n",
    "#    key ranges; this makes each partition locally sorted by the index)\n",
    "ddf_sorted = ddf_sorted.map_partitions(lambda pdf: pdf.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb8f8208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_with_key.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c073d6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_sorted.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd56b584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           tokens quality  \\\n",
      "rand_key                                                                    \n",
      "2877534930248   [23889, 370, 17329, 370, 12944, 341, 719, 241,...    high   \n",
      "2922431929578   [469, 12492, 194, 27, 289, 20729, 3876, 401, 6...    high   \n",
      "6810374980414   [57460, 13718, 18564, 234, 194, 29, 469, 1810,...    high   \n",
      "7745255807730   [323, 4255, 2789, 250, 87, 84, 13430, 12514, 1...    high   \n",
      "11080897116137  [11569, 851, 9574, 370, 625, 361, 2459, 1165, ...    high   \n",
      "\n",
      "                  kind   kind2  \n",
      "rand_key                        \n",
      "2877534930248   actual  actual  \n",
      "2922431929578   actual  actual  \n",
      "6810374980414   actual  actual  \n",
      "7745255807730   actual  actual  \n",
      "11080897116137  actual  actual  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ddf_sorted.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87095658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc2e15c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/13 23:37:06 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=high/kind=actual/kind2=actual/CC-MAIN-2020-*.parquet.\n",
      "java.io.FileNotFoundException: File /local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=high/kind=actual/kind2=actual/CC-MAIN-2020-*.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "data_files = \"/local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=high/kind=actual/kind2=actual/CC-MAIN-2020-*.parquet\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"shuffle-by-rand-key\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")  # helps Top-N, skew, etc.\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 1) Load and add a deterministic random key (float in [0,1))\n",
    "df = spark.read.parquet(data_files)\n",
    "df_with_key = df.withColumn(\"rand_key\", F.rand(seed=123456))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208cecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Print a couple of rows in global key order.\n",
    "# This uses Spark's TakeOrdered path (doesn't fully sort 1.5TB just to fetch 5 rows)\n",
    "df_with_key.orderBy(F.col(\"rand_key\").asc()).show(5, truncate=False)\n",
    "\n",
    "# --- Optional: create a scalable globally range-ordered layout by key ---\n",
    "\n",
    "# Range-partition by key and sort within each partition (good for full scans)\n",
    "nparts = 256  # tune for your cluster\n",
    "df_range_sorted = (\n",
    "    df_with_key\n",
    "    # .repartitionByRange(nparts, \"rand_key\")\n",
    "    .sortWithinPartitions(\"rand_key\")\n",
    ")\n",
    "\n",
    "# If you want to materialize this layout for future fast epochs:\n",
    "# df_range_sorted.write.mode(\"overwrite\").parquet(\"/path/to/bucketed_by_rand_key\")\n",
    "\n",
    "# If you still want to peek a few rows from this layout:\n",
    "# (Top-N is still simplest/fastest for just a sample)\n",
    "# df_range_sorted.orderBy(F.col(\"rand_key\").asc()).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97898dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=high/kind=actual/kind2=actual/CC-MAIN-2023-40-part-00012.parquet\n",
      "Row groups: 1\n",
      "  RG   0: rows=224,068  ~439.21 MB\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "path = \"/local/home/dvruette/nemotron_tokenized/data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/data/quality=high/kind=actual/kind2=actual/CC-MAIN-2023-40-part-00012.parquet\"\n",
    "pf = pq.ParquetFile(path)              # metadata only; cheap\n",
    "md = pf.metadata\n",
    "\n",
    "print(f\"File: {path}\")\n",
    "print(f\"Row groups: {md.num_row_groups}\")\n",
    "for i in range(md.num_row_groups):\n",
    "    rg = md.row_group(i)\n",
    "    nrows = rg.num_rows\n",
    "    nbytes = rg.total_byte_size        # approximate physical size on disk\n",
    "    print(f\"  RG {i:>3}: rows={nrows:,}  ~{nbytes/1024/1024:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
